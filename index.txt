
1

Fill & Edit
Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 195–202November 7–11, 2021. ©2021 Association for Computational Linguistics195MeetDot: Videoconferencing with Live Translation CaptionsArkady Arkhangorodsky, Christopher Chu, Scot Fang, Yiqi Huang,Denglin Jiang, Ajay Nagesh, Boliang Zhang, Kevin KnightDiDi Labs4640 Admiralty WayMarina del Rey, CA 90292{arkadyarkhangorodsky,chrischu,scotfang,denglinjiang,yiqihuang,ajaynagesh,boliangzhang,kevinknight}@didiglobal.comAbstractWe presentMeetDot, a videoconferencing sys-tem with live translation captions overlaid onscreen. The system aims to facilitate conversa-tion between people who speak different lan-guages, thereby reducing communication bar-riers  between  multilingual  participants.   Cur-rently,  our  system  supports  speech  and  cap-tions in 4 languages and combines automaticspeech recognition (ASR) and machine trans-lation  (MT)  in  a  cascade.We  use  there-translationstrategy  to  translate  the  streamedspeech, resulting in caption flicker.  Addition-ally, our system has very strict latency require-ments to have acceptable call quality.  We im-plement  several  features  to  enhance  user  ex-perience and reduce their cognitive load, suchas smooth scrolling captions and reducing cap-tion flicker.  The modular architecture allowsus to integrate different ASR and MT servicesin  our  backend.   Our  system  provides  an  in-tegrated  evaluation  suite  to  optimize  key  in-trinsic evaluation metrics such as accuracy, la-tency and erasure. Finally, we present an inno-vative cross-lingual word-guessing game as anextrinsic evaluation metric to measure end-to-end system performance. We plan to make oursystem open-source for research purposes.11  IntroductionAs collaborations across countries is the norm inthe modern workplace, videoconferencing is an in-dispensable part of our working lives. The recentwidespread adoption of remote work has necessi-tated effective online communication tools, espe-cially among people who speak different languages.In this work, we presentMeetDot, a videoconfer-encing solution with live translation captions. Par-ticipants can see an overlaid translation of otherparticipants’  speech  in  their  preferred  language.Currently, we support speech and captions in En-glish, Chinese, Spanish and Portuguese.1The  system  will  be  available  athttps://github.com/didi/meetdotThe  system  is  built  by  cascading  automaticspeech  recognition  (ASR)  and  machine  transla-tion (MT) components. We process the incomingspeech signal in a streaming mode, transcribe it inthe speaker’s language to be used as input to an MTsystem to decode in the listener’s language duringthe call. We have very tight latency requirementsto be able to provide good quality captions in alive video call. Our framework has several featuresto enable a better user experience and reduce thecognitive load on the participants such as smoothpixel-wise  scrolling  of  the  captions,  fading  textthat is likely to change and biased decoding of ma-chine translation output (Arivazhagan et al., 2020)to reduce the flicker of the captions.  In addition,we present preliminary work towards identifyingnamed-entity mentions in speech by interpolatinga pronunciation dictionary with the ASR languagemodel.We present the key metrics to measure the qual-ity of the captions such as accuracy, latency andstability (flicker in captions). Our system providesan integrated evaluation suite to enable fast devel-opment through hill climbing on these metrics. Inaddition to these intrinsic metrics, we present an in-teresting online cross-lingual word guessing game,where one of the participants is given a word thatthey describe and the other participants have toguess the word by reading the captions in theirrespective languages.We present the following as the key contributionsof our work:•A video conference system with live transla-tion of multilingual speech into captions over-laid on participants’ videos. The system hasseveral features to enhance user experience.•A comprehensive evaluation suite closely inte-grated with the system and a set of metrics toreduce latency, caption flicker and accuracy.•A cross-lingual word-guessing game that canbe used as an extrinsic metric to evaluate end-
196Figure 1: MeetDot system architecture. The ASR and MT modules are designed to be plug-and-play with differentservices in the backend (e.g. in-house DiDi MT system/Google MT API). Frontend consists of different “views” -e.g. live captions view - captures audio from another browser window/zoom/system to show translated captions.to-end system performance.We are in the process of releasing the system asopen-source software for the purpose of furtheringresearch in the area.2  Related WorkThe area of simultaneous translation has attracteda lot of attention is recent years.  The recent edi-tions of the Workshop on Automatic SimultaneousTranslation (Wu et al., 2021, 2020) and the tutorialin EMNLP 2020 (Huang et al., 2020) provide us anoverview of the state-of-the-art and challenges inlive translation. Recent technical advances includenewer architectures such as prefix-to-prefix (Maet al., 2019) and adaptive policy methods such asimitation learning (Zheng et al., 2019) and mono-tonic  attention  (Raffel  et  al.,  2017).   The  solu-tions in this space are differentiated into speech-to-speech and speech-to-text, in the former there isa speech synthesis component.  Our work falls inthe latter bucket, since we only display captions tothe user.  Since we have access to separate audio-input channels (one per participant), we do not needspeaker diarization (Park et al., 2021).Re-translationis  a  common  strategy  applied,wherein we translate from scratch every new ex-tended source sentence,  transcribed through theASR module as the participant speaks.  The abil-ity to modify previously displayed captions whennew ASR output is available can lead to flicker indisplayed captions. Following previous work (Ari-vazhagan et al., 2020), we measure this using theerasure metric  and reduce it  using biased  beamsearch during MT decoding. To better capture ouruse case of short exchanges in a meeting scenariocompared to long speech, we introduce additionalmetricsinitial lag,incremental caption lag,meanword burstinessandmax word burstiness.There have been recent working systems in thisspace, such as Ma et al. (2019); Cho et al. (2013);Wang et al. (2016) that provide live captions forsingle-speaker lectures and does not focus on multi-party meetings.  Very recently, there are news re-ports of systems that offer live translations of multi-party meetings (ZDNet, 2021), but their technicaldetails are unclear.3  System DescriptionThe   overall   system   architecture   is   shown   inFigure 1.  It mainly consists of two components(1)Frontend: runs on the user’s computer locally(2)Backend:  runs on the server and consists ofASR and MT modules that interact with the Flaskserver.    The  modular  architecture  allows  us  toswap the ASR and MT components from differentservice providers (such as Google ASR or in-houseKaldi-based ASR services).  These are explainedbelow.MeetDot User Interface:We implemented a sim-ple web-based user interface that allows users tohave meetings with automatically translated cap-tions overlaid. Our interface consists of:- A home page (Figure 2, top-left panel) for creat-ing a meeting room with default settings, or a game(§ 4).- A meeting creation page, where the room settingscan be configured in different ways for experimen-
197Figure 2: MeetDot room creation.  Landing page (top, left panel) Any user can set up a MeetDot room and shareits URL with potential participants (bottom, left panel). Admin users can select parameters that control captioning,speech recognition, and translation (right panel, §3).tation. (Figure 2, right panel)- A meeting landing page, where a user specifiestheir speaking language and caption language (usu-ally the same) before joining the meeting. (Figure2, bottom-left panel)- A meeting page, where the video call takes place(Figure 3)The meeting page features real-time translatedcaptions  overlaid  on  each  speaker’s  video,  dis-played to each user in their selected language (Fig-ure 3).  The user’s spoken language and captionlanguage can also be changed on-the-fly. The meet-ings support full video-conferencing functionality,including microphone and camera toggles, screensharing, and gallery/focused views. Finally, thereis a panel that shows the automatically transcribedand translated conversation history in the user’spreferred language, which can also be saved.MeetDot also supports “Live Translation” modethat can translate audio from any audio feed, e.g.a  microphone  or  the  user’s  own  computer.   Weprovide instructions for the latter, so users can seecaptions for audio in either another browser tab oran ongoing Zoom call or audio from their system.The  frontend  is  implemented  in  Vue,2andthe backend is implemented using Flask.  Video-conferencing functionality is built with WebRTC,3usingDaily.co4as the signaling server to initiatereal-time  audio  and  video  connections.Thefrontend sends audio bytes to the Flask backendthrough  a  WebSocket  connection,  which  routesthem  to  the  speech  recognition  and  translationservices  and  returns  translated  captions  to  bedisplayed.2https://vuejs.org/3https://webrtc.org/4https://www.daily.co/
198Figure  3:  MeetDot  videoconference  interface.   Translated  captions  are  incrementally  updated  (word-by-word,phrase-by-phrase)  on  top  of  participant  videos.   Translations  also  appear  in  the  transcript  panel  (on  right,  notshown), updated utterance-by-utterance. Choosing a caption language (4th button from left at the bottom, in green)displays all captions in that particular language. This depicts the view of the English caption user.Speech Recognition and Machine Translation:Our backend services of ASR and MT are joinedin a cascaded manner.  Each participant’s speechis fed in a streaming fashion to the ASR moduleof the appropriate language selected by the user.Additionally, we show the ASR output as captionsto the speaker as feedback/confirmation to them.Each of the transcribed text returned by ASR isfed to the MT system to translate it into the cap-tion language selected by the reader, from scratch.This strategy is termed asre-translation. Since theASR stream continually returns a revised or an ex-tended string, the input to MT is noisy and willlead to the captions overwritten frequently (termedasflicker) leading to a high cognitive load on thereading.  We employ several techniques to havebetter user experience while they are reading thecaptions (elaborated more below).At present, we support English, Chinese, Span-ish and Portuguese languages for both speech andcaptions. The modular architecture of our systemallows us to plug-and-play different ASR and MTservice components in the backend.  We developtwo  different  ASR  systems  based  on  the  Kaldiframework and WeNet (Yao et al., 2021). We canalso swap either of these to use Google ASR APIinstead. For MT, we have the option of using ourin-house DiDi MT system (Chen et al., 2020) aswell as call Google MT API. For Kaldi ASR, weadapt pre-trained Kaldi models to videoconferenc-ing domain by interpolating the pre-trained lan-guage model with our in-domain language mod-els. For WeNet, we use use theUnified Conformermodel and language model interpolation is plannedfor  future  work.5Following  Arivazhagan  et  al.(2020), we modify the decoding procedure of MTin OpenNMT’s ctranslate toolkit6to mitigate theissue of flicker mentioned above.We   include   several   additional   features   toenhance user experience. We use NVIDIA NeMotoolkit7to punctuate the captions and predict if theword should be capitalized or not, which makesthe captions more readable.   We have an initialnamed  entity  recognition  module,  to  recognizementions of participants’ names in speech whenusing the Kaldi ASR system.  This is performedby interpolating the ASR’s language model with alanguage model trained on a synthesised corpusthat  contains  participants’  names.The  name5We use a pre-trained checkpoint for English ASR andtrained a Chinese ASR model from scratch using the multi-cnand TAL datasets.6https://github.com/OpenNMT/CTranslate27https://github.com/NVIDIA/NeMo
199DirectionSystemsFinalTranslationNormalizedInitialIncrementalMean wordMax wordbleulag (s)erasurelag (s)caption lag (s)burstinessburstinessEn-to-ZhGoogle ASR, Google MT17.815.842.693.820.715.2611.83”Kaldi ASR, DiDi MT19.592.720.332.590.434.349.20”WeNet ASR, DiDi MT23.762.390.212.730.474.769.62Zh-to-EnGoogle ASR, Google MT9.992.330.474.312.125.209.46”Kaldi ASR, DiDi MT7.883.330.732.700.422.426.13”WeNet ASR, DiDi MT9.762.270.372.520.322.425.65Table 1: Baseline results on ourdaily work conversationdataset.  Note that the Google API does not have biased-decoding available to reduce flicker. Metrics are explained in §4. Right 4 metrics are introduced by our work.pronunciation dictionary required by Kaldi ASRis  generated  automatically  by  rules.    Profanityis  detected  using  a  standard  list  of  keywordsand  starred  in  both  ASR  and  translation  output.We  have  an  experimental  module  to  detect  thespeaker’s language automatically (instead of beingmanually set by the user). The advantage of sucha  feature  is  to  allow  code-switching  betweenmultiple languages which is a common behavioramong  multilingual  speakers  (Solorio  and  Liu,2008; Sivasankaran et al., 2018).Captioning Strategies:Here we describe how wedeploy ASR and MT capabilities to create transla-tion captions that are incrementally updated in realtime. Since we display translation captions on topof a speaker’s video feed, we have limited screenreal-estate—for example, 3 lines of 60 characterseach.Each time we receive an incremental ASR up-date (hypothesis extension or revision), we trans-late the updated string from scratch (Arivazhaganet al., 2020). If the translation exceeds the availablereal-estate, we display the longest suffix that fits.ASR also signals utterance boundaries;  we onlysend the current, growing utterance through MT,caching the MT results on previous utterances.The  basic  system  exhibits  large  amounts  offlicker (Niehues et al., 2016, 2018), which we miti-gate with these methods:Translate-k.We only send everykth ASR outputto MT. This results in captions that are more stable,but which update more slowly.Translate-t.Improving on translate-k, we send anASR output to MT if at leasttseconds have elapsedsince the last MT call.Mask-k.We suppress the lastkwords from the MToutput, providing time for the translation to settledown (Cho and Esipova, 2016; Gu et al., 2017; Maet al., 2019; Arivazhagan et al., 2020).  We oftenuseMask-4in practice.Biased MT decoding.We encourage word-by-wordMT decoding to match the string output by the pre-vious MT call (Arivazhagan et al., 2020), avoidingtranslation variations that, while acceptable, unfor-tunately introduce flicker.Preserve linebreaks.When possible, we preventwords  from  jumping  back  &  forth  across  line-breaks.Smooth scrolling.We reduce perception of flickerby scrolling lines smoothly, pixel-wise.4  EvaluationDataset:In  order  to  evaluate  our  system  inthe  most-appropriate  deployment  scenario,  weconstruct an evaluation dataset based on meetingswithin our team. We have a total of 5 meetings, 3meetings involving 7 participants and in Englishand 2 meetings involving 2 participants in Chinese.The  content  of  the  meetings  are  daily  workconversation  and  contains  a  total  of  135  mins(94  mins  English  and  41  mins  Chinese).    Wemanually transcribe and translate these meetingsusing a simple and uniform set of guidelines. Theresulting dataset consists of 494 English utterances(∼11kwords, translated into Chinese) and 183Chinese utterances (∼9.8kcharacters, translatedinto English). We use this dataset to measure ourintrinsic evaluation metrics.Intrinsic metrics:We adopt the following metricsfrom previous work on streaming translation (Pap-ineni et al., 2002; Niehues et al., 2016; Arivazhaganet al., 2020):-Final Bleu. We measure the Bleu MT accuracy offinal, target-language utterances against a referenceset of human translations. Anti-flicker devices that“lock in” partial translations will generally decreasefinal Bleu.-Translation lag. We measure (roughly) the aver-age difference, in seconds, between when a wordwas spoken and when its translation was finalizedon the screen.-Normalized erasure. We quantify flicker asm/n,
200Figure 4: Cross-lingual word guessing game for extrinsic evaluation. Roles: one player is given a word to describein their language, one or more players look at the captions displayed to guess the correct word. Screenshot shownis that of the describer of the word. Word is“eyelash”(left panel) - the participants communicate hints and guessesthroughMeetDottranslation captions, and the system itself spots correct guesses.the number of wordsmthat get erased during theproduction of ann-word utterance.To support our videoconferencing application,we introduce other intrinsic metrics:-Initial lag. In videoconferencing, we find it valu-able to generate translation captions immediatelyafter a speaker begins, even if the initial transla-tion involves some flicker.  Here we measure thetime between initial speaking time and the first dis-played word. We improve initial lag by adopting aMask-0policy at the start of an utterance, transi-tioning to our usualMask-4policy later.-Incremental caption lag.  The average time be-tween caption updates.-Mean (& Max) word burstiness. The mean num-ber of words or characters for Chinese (maximum,respectively) that are added/subtracted in a singlecaption update (for a given utterance’s translation–averaged over all utterances, respectively).In Table 1,  we present baseline results of thevarious intrinsic metrics for the English to Chineseand Chinese to English systems. We present resultsfor 3 different module combinations, namely, (i)Google API8for ASR9and MT10(ii) Kaldi ASRand  DiDi  MT  (iii)  WeNet  ASR  and  DiDi  MT.From the results table, we would like to highlightthat the biased decoding modification for machinetranslation has a positive impact on several metrics8Accessed on 2021-09-09.9https://cloud.google.com/speech-to-text10https://cloud.google.com/translatesuch  as  translation  lag,  normalized  erasure  andword burstiness. Biased decoding is absent in theGoogle  MT  API,  hence  has  higher  numbers  inthese metrics.   This results in increasing flickerleading  to  a  poorer  user  experience.   Our  finalbleu  score  when  using  WeNet  ASR  and  DiDiMT is several points better than Google ASR andGoogle MT in the English to Chinese directionand  has  comparable  performance  in  Chinese  toEnglish direction.  ASR system’s performance isan important factor in a cascaded system.  KaldiASR has a word error rate (WER) rate of 43.88for English (character error rate (CER) of 49.75for  Chinese,  respectively)  compared  to  WeNetASR’s WER of 34.74 (CER of 38.03 for Chinese,respectively). This has a direct impact on final bleuscores as seen from the results.Cross-lingual word guessing game:Extrinsicmetrics for speech translation are not as popular asintrinsic ones (Arivazhagan et al., 2020; Niehueset al., 2016).  However, given the broad range oftechniques for displaying translation captions, wewould like to measure things that are closer to theuser experience.Here, we introduce a cross-lingual, cooperativeword game for A/B testing different captioning al-gorithms.  Players who speak different languagesuseMeetDot’s translation captions to communicatewith each other. If the players obtain higher scoresunder one condition, we conclude that their com-munication is, in some way, made more effective
201and efficient.The game is a variation onTaboo,11in which oneplayer receives a secret word (such as “racoon” or“scary”) and must describe it without mentioningthe word or variant of it. The other player tries toguess the word.  The players are awarded a pointfor every word guessed in a 4-minute period. Thefirst player is allowed to skip upto three words.In our variation, the first player may receive aChinese word and describe it in Chinese,  whilethe second player sees English captions and makesguesses in English. When translation is quick, ac-curate, and readable, players score higher.We  design  our  bilingual  wordlists  to  containwords with limited ambiguity.  This way, we areable to build a reliable, automatic scorer that re-wards players and advances them to the next word.We also implement a competitive, multi-playerversion of the game, where players are assignedpoints for making correct guesses faster than others,and for giving clues that lead to fast guesses.5  Conclusion and Future WorkWe  describeMeetDot,  a  videoconferencing  sys-tem with live translation captions, along with itscomponents:  UI, ASR, MT, and captioning.  Weimplement  an  evaluation  suite  that  allows  us  toaccurately compute metrics from the sequence ofcaptions that users would see. We also describe across-lingual word game for A/B testing differentcaptioning algorithms and conditions.Our future work includes improved ASR/MT,extrinsic testing, and an open source release. Ouroverall goal is to provide a platform for developingtranslation captions that are accurate and “rightbehind you”.ReferencesNaveen  Arivazhagan,  Colin  Cherry,  I  Te,  WolfgangMacherey,  Pallavi  Baljekar,  and  George  F.  Foster.2020.    Re-translation  strategies  for  long  form,  si-multaneous,  spoken language translation.   InProc.ICASSP.Tanfang Chen, Weiwei Wang, Wenyang Wei, Xing Shi,Xiangang Li,  Jieping  Ye,  and Kevin Knight.  2020.DiDi’s  machine  translation  system  for  WMT2020.InProc. WMT.Eunah   Cho,   C.   Fügen,   T.   Herrmann,   Kevin   Kil-gour, M. Mediani, Christian Mohr, J. Niehues, Kay11https://www.hasbro.com/common/instruct/Taboo(2000).PDFRottmann, Christian Saam, S. Stüker, and A. Waibel.2013. A real-world system for simultaneous transla-tion of german lectures. InINTERSPEECH.Kyunghyun Cho and Masha Esipova. 2016.  Can neu-ral machine translation do simultaneous translation?ArXiv, abs/1606.02012.Jiatao Gu, Graham Neubig, Kyunghyun Cho, and Vic-tor O.K. Li. 2017.  Learning to translate in real-timewith neural machine translation. InProc. EACL.Liang Huang, Colin Cherry, Mingbo Ma, Naveen Ari-vazhagan,  and  Zhongjun  He.  2020.   Simultaneoustranslation. InProc. EMNLP: Tutorial Abstracts.Mingbo Ma, Liang Huang, Hao Xiong, Renjie Zheng,Kaibo  Liu,   Baigong  Zheng,   Chuanqiang  Zhang,Zhongjun He, Hairong Liu, Xing Li, Hua Wu, andHaifeng Wang. 2019.  STACL: Simultaneous trans-lation with implicit anticipation and controllable la-tency  using  prefix-to-prefix  framework.InProc.ACL.J.   Niehues,    T.   Nguyen,    Eunah   Cho,    Thanh-LeHa,  Kevin  Kilgour,  M.  Müller,  Matthias  Sperber,S. Stüker, and A. Waibel. 2016.  Dynamic transcrip-tion for low-latency speech translation.   InINTER-SPEECH.J. Niehues, Ngoc-Quan Pham, Thanh-Le Ha, MatthiasSperber, and A. Waibel. 2018.  Low-latency neuralspeech translation. InINTERSPEECH.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002.  Bleu:  a method for automatic eval-uation of machine translation. InProc. ACL.Tae   Jin   Park,   Naoyuki   Kanda,   Dimitrios   Dimitri-adis,  Kyu  J.  Han,  Shinji  Watanabe,  and  ShrikanthNarayanan.   2021.A   review   of   speaker   di-arization:Recent   advances   with   deep   learning.arXiv:2101.09624.Colin Raffel, Minh-Thang Luong, Peter J. Liu, Ron J.Weiss,  and Douglas Eck. 2017.   Online and linear-time  attention  by  enforcing  monotonic  alignments.InProc. ICML.Sunit   Sivasankaran,    Brij   Mohan   Lal   Srivastava,Sunayana Sitaram, Kalika Bali, and Monojit Choud-hury.  2018.Phone  merging  for  code-switchedspeech recognition.  InWorkshop on ComputationalApproaches to Linguistic Code Switching.Thamar Solorio and Yang Liu. 2008.  Learning to pre-dict code-switching points. InProc. EMNLP.Xiaolin Wang, Andrew Finch, Masao Utiyama, and Ei-ichiro Sumita. 2016. A prototype automatic simulta-neous interpretation system. InProc. COLING: Sys-tem Demonstrations.Hua  Wu,  Colin  Cherry,  Liang  Huang,  Zhongjun  He,Qun Liu, Maha Elbayad, Mark Liberman, HaifengWang,  Mingbo  Ma,  and  Ruiqing  Zhang,  editors.2021.Proc. Second Workshop on Automatic Simul-taneous Translation.
202Hua Wu,  Collin Cherry,  Liang Huang,  Zhongjun He,Mark Liberman, James Cross, and Yang Liu, editors.2020.Proc. First Workshop on Automatic Simulta-neous Translation.Zhuoyuan  Yao,  Di  Wu,  Xiong  Wang,  Binbin  Zhang,Fan Yu, Chao Yang, Zhendong Peng, Xiaoyu Chen,Lei  Xie,  and  Xin  Lei.  2021.    Wenet:   Productionoriented  streaming  and  non-streaming  end-to-endspeech recognition toolkit. InINTERSPEECH.ZDNet.  2021.   Cisco’s  webex  debuts  real-time  trans-lation  from  english  to  100+  languages.https://tinyurl.com/bwvfm8ka.   Accessed:  2021-06-30.Baigong  Zheng,  Renjie  Zheng,  M.  Ma,  and  LiangHuang. 2019. Simultaneous translation with flexiblepolicy via restricted imitation learning. InACL.
